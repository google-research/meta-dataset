{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5O1UdsY202_",
    "colab_type": "text"
   },
   "source": [
    "##### Copyright 2019 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zy3bZKW82xP9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXhLzrXN27af"
   },
   "source": [
    "# Using the Meta-Dataset Data Pipeline\n",
    "This notebook shows how to use `meta_dataset`’s input pipeline to sample data for the Meta-Dataset benchmark. There are two main ways in which data is sampled:\n",
    "1. **episodic**:  Returns N-way classification *episodes*, which contain a *support* (training) set and a *query* (test) set. The number of classes (N) may vary from episode to episode.\n",
    "2. **batch**:  Returns batches of images and their corresponding label, sampled from all available classes.\n",
    "\n",
    "We first import `meta_dataset` and other required packages, and define utility functions for visualization. We’ll make use of `meta_dataset.data.learning_spec` and `meta_dataset.data.pipeline`; their purpose will be made clear later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "ZyMqBhZIxPQD"
   },
   "outputs": [],
   "source": [
    "#@title Imports and Utility Functions\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import gin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import six.moves.cPickle as pkl\n",
    "import tensorflow as tf\n",
    "from meta_dataset.data import learning_spec\n",
    "from meta_dataset.data import pipeline\n",
    "\n",
    "def plot_episode(support_images, support_class_ids, query_images,\n",
    "                 query_class_ids, size_multiplier=1, max_imgs_per_col=10,\n",
    "                 max_imgs_per_row=10):\n",
    "  for name, images, class_ids in zip(('Support', 'Query'),\n",
    "                                     (support_images, query_images),\n",
    "                                     (support_class_ids, query_class_ids)):\n",
    "    n_samples_per_class = Counter(class_ids)\n",
    "    n_samples_per_class = {k: min(v, max_imgs_per_col)\n",
    "                           for k, v in n_samples_per_class.items()}\n",
    "    id_plot_index_map = {k: i for i, k\n",
    "                         in enumerate(n_samples_per_class.keys())}\n",
    "    num_classes = min(max_imgs_per_row, len(n_samples_per_class.keys()))\n",
    "    max_n_sample = max(n_samples_per_class.values())\n",
    "    figwidth = max_n_sample\n",
    "    figheight = num_classes\n",
    "    if name == 'Support':\n",
    "      print('#Classes: %d' % len(n_samples_per_class.keys()))\n",
    "    figsize = (figheight * size_multiplier, figwidth * size_multiplier)\n",
    "    fig, axarr = plt.subplots(\n",
    "        figwidth, figheight, figsize=figsize)\n",
    "    fig.suptitle('%s Set' % name, size='20')\n",
    "    fig.tight_layout(pad=3, w_pad=0.1, h_pad=0.1)\n",
    "    reverse_id_map = {v: k for k, v in id_plot_index_map.items()}\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "      ax.patch.set_alpha(0)\n",
    "      # Print the class ids, this is needed since, we want to set the x axis\n",
    "      # even there is no picture.\n",
    "      ax.set(xlabel=reverse_id_map[i % figheight], xticks=[], yticks=[])\n",
    "      ax.label_outer()\n",
    "    for image, class_id in zip(images, class_ids):\n",
    "      # First decrement by one to find last spot for the class id.\n",
    "      n_samples_per_class[class_id] -= 1\n",
    "      # If class column is filled or not represented: pass.\n",
    "      if (n_samples_per_class[class_id] < 0 or\n",
    "          id_plot_index_map[class_id] >= max_imgs_per_row):\n",
    "        continue\n",
    "      # If width or height is 1, then axarr is a vector.\n",
    "      if axarr.ndim == 1:\n",
    "        ax = axarr[n_samples_per_class[class_id]\n",
    "                   if figheight == 1 else id_plot_index_map[class_id]]\n",
    "      else:\n",
    "        ax = axarr[n_samples_per_class[class_id], id_plot_index_map[class_id]]\n",
    "      ax.imshow(image / 2 + 0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_batch(images, labels, size_multiplier=1):\n",
    "  num_examples = len(labels)\n",
    "  figwidth = np.ceil(np.sqrt(num_examples)).astype('int32')\n",
    "  figheight = num_examples // figwidth\n",
    "  figsize = (figwidth * size_multiplier, (figheight + 1.5) * size_multiplier)\n",
    "  _, axarr = plt.subplots(figwidth, figheight, dpi=300, figsize=figsize)\n",
    "\n",
    "  for i, ax in enumerate(axarr.transpose().ravel()):\n",
    "    # Images are between -1 and 1.\n",
    "    ax.imshow(images[i] / 2 + 0.5)\n",
    "    ax.set(xlabel=labels[i], xticks=[], yticks=[])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOn_YZdqPIv5"
   },
   "source": [
    "# Primers\n",
    "1. Download your data and process it as explained in [link](https://github.com/google-research/meta-dataset/blob/master/README.md#downloading-and-converting-datasets). Set `BASE_PATH` pointing the processed tf-records (`$RECORDS` in the conversion instructions).\n",
    "2. `meta_dataset` supports many different setting for sampling data. We use [gin-config](https://github.com/google/gin-config) to control default parameters of our functions. You can go to default gin file we are pointing and see the default values.\n",
    "3. You can use `meta_dataset` in **eager** or **graph** mode.\n",
    "4. Let's write a generator that makes the right calls to return data from dataset. `dataset.make_one_shot_iterator()` returns an iterator where each element is an episode.\n",
    "4. SPLIT is used to define which part of the meta-split is going to be used. Different splits have different classes and the details on how they are created can be found in the [paper](https://arxiv.org/abs/1903.03096)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_di9Tczj8joM"
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "BASE_PATH = '/path/to/records'\n",
    "GIN_FILE_PATH = 'meta_dataset/learn/gin/setups/data_config.gin'\n",
    "# 2\n",
    "gin.parse_config_file(GIN_FILE_PATH)\n",
    "# 3\n",
    "# Comment out to disable eager execution.\n",
    "tf.enable_eager_execution()\n",
    "# 4\n",
    "def iterate_dataset(dataset, n):\n",
    "  if not tf.executing_eagerly():\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    with tf.Session() as sess:\n",
    "      for idx in range(n):\n",
    "        yield idx, sess.run(next_element)\n",
    "  else:\n",
    "    for idx, episode in enumerate(dataset):\n",
    "      if idx == n:\n",
    "        break\n",
    "      yield idx, episode\n",
    "# 5\n",
    "SPLIT = learning_spec.Split.TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pn6ndPMhxs8W"
   },
   "source": [
    "# Reading datasets\n",
    "In order to sample data, we need to read the dataset_spec files for each dataset. Following snippet reads those files into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0uU6WrbxsMa"
   },
   "outputs": [],
   "source": [
    "ALL_DATASETS = ['aircraft', 'cu_birds', 'dtd', 'fungi', 'ilsvrc_2012',\n",
    "                'omniglot', 'quickdraw', 'vgg_flower']\n",
    "\n",
    "all_dataset_specs = []\n",
    "for dataset_name in ALL_DATASETS:\n",
    "  dataset_records_path = os.path.join(BASE_PATH, dataset_name)\n",
    "  ds_spec_path = os.path.join(dataset_records_path, 'dataset_spec.pkl')\n",
    "  with tf.gfile.Open(ds_spec_path, 'rb') as f:\n",
    "    dataset_spec = pkl.load(f)\n",
    "  dataset_spec = dataset_spec._replace(path=dataset_records_path)\n",
    "  all_dataset_specs.append(dataset_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7p448EXYxwbb"
   },
   "source": [
    "# (1) Episodic Mode\n",
    "`meta_dataset` uses [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) API and it takes one call to `pipeline.make_multisource_episode_pipeline()`. We loaded or defined most of the variables used during this call above. The remaining parameters are explained below:\n",
    "\n",
    "- **use_bilevel_ontology_list**:  This is a list of booleans indicating whether corresponding dataset in `ALL_DATASETS` should use bilevel ontology. Omniglot is set up with a hierarchy with two level: the alphabet (Latin, Inuktitut...), and the character (with 20 examples per character).\n",
    "The flag means that each episode will contain classes from a single alphabet. \n",
    "- **use_dag_ontology_list**:  This is a list of booleans indicating whether corresponding dataset in `ALL_DATASETS` should use dag_ontology. Same idea for ImageNet, except it uses the hierarchical sampling procedure described in the article.\n",
    "- **image_size**: All images from various datasets are down or upsampled to the same size. This is the flag controls the edge size of the square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPlnBWwkwuGP"
   },
   "outputs": [],
   "source": [
    "use_bilevel_ontology_list = [False]*len(ALL_DATASETS)\n",
    "use_dag_ontology_list = [False]*len(ALL_DATASETS)\n",
    "# Enable ontology aware sampling for Omniglot and ImageNet. \n",
    "use_bilevel_ontology_list[5] = True\n",
    "use_dag_ontology_list[4] = True\n",
    "\n",
    "dataset = pipeline.make_multisource_episode_pipeline(\n",
    "    dataset_spec_list=all_dataset_specs,\n",
    "    use_dag_ontology_list=use_dag_ontology_list,\n",
    "    use_bilevel_ontology_list=use_bilevel_ontology_list,\n",
    "    split=SPLIT, image_size=84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BN66UXO79Bo2"
   },
   "source": [
    "## Using Dataset\n",
    "1. Episodes consist of support and query sets and we want to learn to classify images at the query set correctly given the support images. For both support and query set we have `images`, `labels` and `class_ids`. Labels are transformed class_ids offset to zero, so that global class_ids are set to \\[0, N\\] where N is the number of classes in an episode.\n",
    "2. As one can see the number of images in query set and support set is different. Images are scaled, copied into 84\\*84\\*3 tensors.\n",
    "3. It easy to convert tensors of the episode into numpy arrays and use them outside of the Tensorflow framework.\n",
    "4. Classes might have different number of samples in the support set, whereas each class has 10 samples in the query set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 163
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22702,
     "status": "ok",
     "timestamp": 1554851710603,
     "user": {
      "displayName": "Utku Evci",
      "photoUrl": "https://lh6.googleusercontent.com/-5Bz0PqZmz_I/AAAAAAAAAAI/AAAAAAAAAMo/lmgAzNPYn1U/s64/photo.jpg",
      "userId": "01088181649958641579"
     },
     "user_tz": 240
    },
    "id": "lomtjv9rw5WP",
    "outputId": "158edcaf-c05d-4eaf-9592-c190be7e798f"
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "idx, episode = next(iterate_dataset(dataset, 1))\n",
    "\n",
    "# 2\n",
    "for t, name in zip(episode,\n",
    "                   ['support_images', 'support_labels', 'support_class_ids',\n",
    "                    'query_images', 'query_labels', 'query_class_ids']):\n",
    "  print(name, t.shape)\n",
    "\n",
    "# 3\n",
    "episode = [a.numpy() for a in episode]\n",
    "\n",
    "# 4\n",
    "support_class_ids, query_class_ids = episode[2], episode[5]\n",
    "print(Counter(support_class_ids))\n",
    "print(Counter(query_class_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxdVUqJiWmTX"
   },
   "source": [
    "## Visualizing Episodes\n",
    "Let's visualize the episodes. \n",
    "\n",
    "- Support and query set for each episode plotted sequentially. Set N_EPISODES to control number of episodes visualized.\n",
    "- Each episode is sampled from a single dataset and include N different classes. Each class might have different number of samples in support set, whereas number of images in query set is fixed. We limit number of classes and images per class to 10 in order to create legible plots. Actual episodes might have more classes and samples.  \n",
    "- Each column represents a distinct class and dataset specific class ids are plotted on the x_axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 3663
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25841,
     "status": "ok",
     "timestamp": 1554851889070,
     "user": {
      "displayName": "Utku Evci",
      "photoUrl": "https://lh6.googleusercontent.com/-5Bz0PqZmz_I/AAAAAAAAAAI/AAAAAAAAAMo/lmgAzNPYn1U/s64/photo.jpg",
      "userId": "01088181649958641579"
     },
     "user_tz": 240
    },
    "id": "9v2ePLTkoZlE",
    "outputId": "2f98cbb1-b58d-42e8-ebcf-c3645f02058d"
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "N_EPISODES=2\n",
    "# 2, 3\n",
    "for idx, episode in iterate_dataset(dataset, N_EPISODES):\n",
    "  print('Episode id: %d' % idx)\n",
    "  episode = [a.numpy() for a in episode]\n",
    "  plot_episode(support_images=episode[0], support_class_ids=episode[2],\n",
    "               query_images=episode[3], query_class_ids=episode[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pL2AZ5gx3cDS"
   },
   "source": [
    "# (2) Batch Mode\n",
    "Second mode that `meta_dataset` library provides is the batch mode, where one can sample batches from the list of  datasets in a non-episodic manner and use it to train baseline models. There are couple things to note here:\n",
    "\n",
    "- Each batch is sampled from a different dataset.\n",
    "- `ADD_DATASET_OFFSET` controls whether the class_id's returned by the iterator overlaps among different datasets or not. A dataset specific offset is added in order to make returned ids unique.\n",
    "- `make_multisource_batch_pipeline()` creates a `tf.data.Dataset` object that returns batches from multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYY5zd_S6uG6"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "ADD_DATASET_OFFSET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37308,
     "status": "ok",
     "timestamp": 1554851202768,
     "user": {
      "displayName": "Utku Evci",
      "photoUrl": "https://lh6.googleusercontent.com/-5Bz0PqZmz_I/AAAAAAAAAAI/AAAAAAAAAMo/lmgAzNPYn1U/s64/photo.jpg",
      "userId": "01088181649958641579"
     },
     "user_tz": 240
    },
    "id": "BgkTLKKXPh8M",
    "outputId": "695c89e3-786a-43c1-92a6-866f4bc9bb04"
   },
   "outputs": [],
   "source": [
    "dataset = pipeline.make_multisource_batch_pipeline(\n",
    "    dataset_spec_list=all_dataset_specs, batch_size=BATCH_SIZE, split=SPLIT,\n",
    "    image_size=84, add_dataset_offset=ADD_DATASET_OFFSET)\n",
    "\n",
    "for idx, (images, labels) in iterate_dataset(dataset, 1):\n",
    "  print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1010
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32201,
     "status": "ok",
     "timestamp": 1554851240654,
     "user": {
      "displayName": "Utku Evci",
      "photoUrl": "https://lh6.googleusercontent.com/-5Bz0PqZmz_I/AAAAAAAAAAI/AAAAAAAAAMo/lmgAzNPYn1U/s64/photo.jpg",
      "userId": "01088181649958641579"
     },
     "user_tz": 240
    },
    "id": "7hGjt6GGonAz",
    "outputId": "a7f39454-c8ef-4ed2-92c4-d3a4f0099077"
   },
   "outputs": [],
   "source": [
    "N_BATCH = 2\n",
    "for idx, batch in iterate_dataset(dataset, N_BATCH):\n",
    "  print('Batch-%d' % idx)\n",
    "  plot_batch(*map(lambda a: a.numpy(), batch), size_multiplier=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Fixing Ways and Shots\n",
    "1. `meta_dataset` library provides option to set number of classes/samples per episode. There are 3 main flags you can set. \n",
    "    - **NUM_WAYS**: Fixes the # classes per episode. We would still get variable number of samples per class in the support set.\n",
    "    - **NUM_SUPPORT**: Fixes # samples per class in the support set.\n",
    "    - **NUM_SUPPORT**: Fixes # samples per class in the query set.\n",
    "2. If we want to use fixed `num_ways`, we have to disable ontology based sampling for omniglot and imagenet. We advise using single dataset for using this feature, since using multiple datasets is not supported/tested. In this notebook, we are using Quick, Draw! Dataset.\n",
    "3. We sample episodes and visualize them as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8raM-sad6Igu"
   },
   "outputs": [],
   "source": [
    "#1\n",
    "NUM_WAYS = 8\n",
    "NUM_SUPPORT = 3\n",
    "NUM_QUERY = 5\n",
    "\n",
    "#2\n",
    "use_bilevel_ontology_list = [False]*len(ALL_DATASETS)\n",
    "use_dag_ontology_list = [False]*len(ALL_DATASETS)\n",
    "quickdraw_spec = [all_dataset_specs[6]]\n",
    "#3\n",
    "dataset = pipeline.make_multisource_episode_pipeline(\n",
    "    dataset_spec_list=quickdraw_spec, use_dag_ontology_list=[False],\n",
    "    use_bilevel_ontology_list=use_bilevel_ontology_list, split=SPLIT,\n",
    "    image_size=84, num_ways=NUM_WAYS, num_support=NUM_SUPPORT,\n",
    "    num_query=NUM_QUERY)\n",
    "\n",
    "N_EPISODES = 2\n",
    "for idx, episode in iterate_dataset(dataset, N_EPISODES):\n",
    "  print('Episode id: %d' % idx)\n",
    "  episode = [a.numpy() for a in episode]\n",
    "  plot_episode(support_images=episode[0], support_class_ids=episode[2],\n",
    "               query_images=episode[3], query_class_ids=episode[5])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e5O1UdsY202_"
   ],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "Intro to Metadataset.ipynb",
   "provenance": [
    {
     "file_id": "1z83txZ92A3930dqYb8kw-NF8IUhQIHk0",
     "timestamp": 1554403615924
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
